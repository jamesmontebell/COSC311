{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 311: Introduction to Data Visualization and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Dr. Shuangquan (Peter) Wang\n",
    "\n",
    "Email: spwang@salisbury.edu\n",
    "\n",
    "Department of Computer Science, Salisbury University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4_Statistics & Probability\n",
    "\n",
    "## 2. Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents of this note refer to 1) Dr. Joe Anderson's teaching materials; 2) textbook \"Data Science from Scratch\";**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>All rights reserved. Dissemination or sale of any part of this note is NOT permitted.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Random Variables\n",
    "\n",
    "## Review of Some Concepts\n",
    "\n",
    "### Probability\n",
    "\n",
    "We use the language of probability to give structure to uncertain and complex processes. When we talk about a \"random variable\" we mean an observable phenomenon that takes values in a particular universe of possibility.\n",
    "\n",
    "For example, flipping a coin can take values in $\\{H, T\\}$, and to each we assign a probability $P(H)$ and $P(T)$. $P(H)$ means the probability of the event $H$ happens, and $P(T)$ means the probability of the event $T$ happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence and Independence\n",
    "\n",
    "We say two events E and F are *dependent* if knowing something about whether E happens gives us information about whether F happens (and vice versa). Otherwise, they are independent.\n",
    "\n",
    "When two events E and F are *independent*, the probability that they both happen is the product of the probabilities that each one happens:\n",
    "\n",
    "$P(E,F) = P(E) * P(F)$\n",
    "\n",
    "For example, let's flip a coin twice. The probability of \"first flip heads\" is 1/2, and the probability of \"both flips heads\" is 1/2 * 1/2 = 1/4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "If two events E and F are not necessarily independent, then we define the probability of E \"conditional on F\" as $P(E|F) = \\frac{P(E,F)}{P(F)}$.\n",
    "\n",
    "Question: what is the value of $P(E|F)$ if E and F are independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is a way of \"reversing\" conditional probabilities. If we want to know the probability of event E conditional on event F, $P(E|F)$, but we only know the probability of event F conditional on event E, $P(F|E)$. We can calculate $P(E|F)$ as following:\n",
    "\n",
    "$$P(E|F) = \\frac{P(E,F)}{P(F)} = \\frac{P(F|E)*P(E)}{P(F)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variables\n",
    "\n",
    "A *random variable* is a variable whose possible values have an associated probability distribution.\n",
    "\n",
    "A simple random variable equals 1 if a coin flip turns up heads and 0 if the flip turns up tails (A more complicated one may measure the number of heads you observe when flipping a coin 10 times).\n",
    "\n",
    "*expected value* of a random variable is the average of its values weighted by their probabilities. The coin flip variable has an expected value of (0 * 0.5 + 1 * 0.5) = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Random Variables\n",
    "\n",
    "The random variable $X$ is said to be discrete if sample space (i.e. possible values) $\\Omega$ is countable (either finite or countably infinite).\n",
    "\n",
    "Examples:\n",
    "- flip of a coin\n",
    "- number of people in a class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the cumulative probability function (cdf) for discrete random variable $X$ as\n",
    "$$ P(X \\leq t) = \\sum_{\\omega \\leq t} P(X = \\omega) $$\n",
    "i.e. the sum of probabilities for all values at most $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Random Variables\n",
    "\n",
    "A random variable $X$ is continuous if the associated sample space (i.e. possible values) $\\Omega$ is uncountably infinite. \n",
    "\n",
    "For example:\n",
    "- A random draw from $(0,1)$\n",
    "- Temperature measured on a day (with infinite precision)\n",
    "- Measuring speed of cars on a road at a certain location\n",
    "\n",
    "We then define the cdf for continuous random variable $X$ as $P(X \\leq t)$, which is usually defined for that specific random variable/distribution. However, most common continuous random variables have a Probability Density Function (pdf) called $f_X(x)$ so that \n",
    "$$ P(X \\leq t) = \\int_{-\\infty}^{t} f_X(x) dx $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform distribution\n",
    "\n",
    "Look at uniform draw from interval $(0,1)$. Call this draw $X$. The distribution is denoted $U(0,1)$ when all values have equal probability, i.e. the uniform distribution on $(0,1)$.\n",
    "\n",
    "The pdf of this is then \n",
    "$$ f_X(x) = \n",
    "\\begin{cases}\n",
    "0 \\text{ if } x \\not \\in (0,1) \\\\\n",
    "1 \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "because we still need the law of total probability:\n",
    "$$ \\int_{-\\infty}^{\\infty} f_X(x) dx = \\int_{0}^{1} f_X(x) dx = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general for interval $(a,b)$ the uniform distribution has pdf \n",
    "$$ f(x) =\n",
    "\\begin{cases}\n",
    "0 \\text{ if } x \\not \\in (a,b) \\\\\n",
    "\\frac{1}{b-a} \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_pdf(x,a,b):\n",
    "    return 1/(b-a) if a <= x and x <= b else 0\n",
    "\n",
    "def uniform_cdf(t,a,b):\n",
    "    if t < a:\n",
    "        return 0\n",
    "    if t > b:\n",
    "        return 1\n",
    "    return (t-a)/(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xs = np.arange(-2,2,0.01)\n",
    "ys = [uniform_pdf(x,0,1) for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "ys = [uniform_cdf(x,0,1) for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "plt.legend(['pdf of U(0,1)', 'cdf of U(0,1)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for random variable $X$ distributed as $U(a,b)$, calculate the cdf:\n",
    "First, suppose $t \\in (a,b)$.\n",
    "\n",
    "$$ P(X \\leq t) = \\int_{-\\infty}^{t} f_X(x) dx = \\int_{a}^{t} \\frac{1}{b-a} dx = \\frac{1}{b-a} x \\bigg \\vert_{a}^{t} = \\frac{t}{b-a} - \\frac{a}{b-a} = \\frac{t-a}{b-a}$$\n",
    "so final form:\n",
    "\n",
    "$$ P(X \\leq t)  = \n",
    "\\begin{cases}\n",
    "0 \\text{ if } t <= a \\\\\n",
    "\\frac{t-a}{b-a} \\text{ if } t \\in (a,b) \\\\\n",
    "1 \\text{ if } t > b \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calculating the mean of $X$ when distributed as $U(0,1)$:\n",
    "\n",
    "$$\\mathbb{E}X = \\int_{-\\infty}^{\\infty} x f_X(x) dx = \\int_{0}^{1} x dx = \\frac{x^2}{2} \\bigg \\vert_{0}^{1} = 1/2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about uniform distribution in interval (0,2)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xs = np.arange(-2,3,0.01)\n",
    "ys = [uniform_pdf(x,0,2) for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "ys = [uniform_cdf(x,0,2) for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "plt.legend(['pdf of U(0,2)', 'cdf of U(0,2)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "\n",
    "Also called the Gaussian distribution (a bell curve-shaped distribution), the pdf is\n",
    "$$ f_X(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left( -\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right) $$\n",
    "and we normally abbreviate this distribution as $ \\mathcal{N}(\\mu, \\sigma^2) $.\n",
    "In this formulation, the mean is $\\mu$ that indicates where the bell is centered; the standard deviation $\\sigma$ (or variance $\\sigma^2$) indicates how wide the bell is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    return (1/(sigma*np.sqrt(2 * np.pi))) * np.exp(-1*(x - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "def normal_cdf(x, mu, sigma):\n",
    "    return (1 + math.erf((x-mu)/(sigma*np.sqrt(2))))/2\n",
    "# math.erf(x): Return the error function at x.\n",
    "# The erf() function can be used to compute traditional statistical functions \n",
    "# such as the cumulative standard normal distribution\n",
    "# https://docs.python.org/3/library/math.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.01)\n",
    "plt.plot(xs, [normal_pdf(x,0,1) for x in xs], 'b', label='mu=0 sigma=1')\n",
    "plt.plot(xs, [normal_pdf(x,0,1.5) for x in xs], 'g--', label='mu=0 sigma=1.5')\n",
    "plt.plot(xs, [normal_pdf(x,0,2) for x in xs], 'r:', label='mu=0 sigma=2')\n",
    "plt.plot(xs, [normal_pdf(x,0,0.5) for x in xs], 'y', label='mu=0 sigma=0.5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.01)\n",
    "plt.plot(xs, [normal_cdf(x,0,1) for x in xs], 'b', label='mu=0 sigma=1')\n",
    "plt.plot(xs, [normal_cdf(x,0,1.5) for x in xs], 'g--', label='mu=0 sigma=1.5')\n",
    "plt.plot(xs, [normal_cdf(x,0,2) for x in xs], 'r:', label='mu=0 sigma=2')\n",
    "plt.plot(xs, [normal_cdf(x,0,0.5) for x in xs], 'y', label='mu=0 sigma=0.5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.01)\n",
    "plt.plot(xs, [normal_pdf(x,0,1) for x in xs], 'b:', label='pdf: mu=0 sigma=1')\n",
    "plt.plot(xs, [normal_cdf(x,0,1) for x in xs], 'b', label='cdf: mu=0 sigma=1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.01)\n",
    "plt.plot(xs, [normal_pdf(x,0,1) for x in xs], 'b:', label='mu=0 sigma=1')\n",
    "plt.plot(xs, [normal_pdf(x,1,1) for x in xs], 'b', label='mu=1 sigma=1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Limit Theorem\n",
    "\n",
    "The normal distribution is very useful because of the central limit theorem, which says that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally distributed.\n",
    "\n",
    "Let $X$ be *any* random variable with mean $\\mu$ and variance $\\sigma^2$.\n",
    "Then if you have $x_1, x_2, \\dots, x_N$ iid (independent and identially distributed) copies/observations of $X$, the quantity\n",
    "$$ \\bar{x} = \\frac{1}{N} (x_1 + x_2 + \\dots + x_N) $$\n",
    "will, if $N$ is large enough, behave like $\\mathcal{N}\\left( \\mu, \\frac{\\sigma^2}{N} \\right)$.\n",
    "That is, the empirical mean (interpreted now as a random variable) will look normal, if enough samples are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: flipping a coin.\n",
    "\n",
    "We're given a (maybe unfair) coin that flips heads with probabilitiy $p$ and tails with probability $1-p$.\n",
    "We want to estimate $p$ by flipping the coin ourselves.\n",
    "\n",
    "So lets repeat the following experiment: Flip the coin $n$ times, calculate the average.\n",
    "But after repeating this, look at the distribution of values. What will it look like?\n",
    "\n",
    "A coin flip is usually referred to as a Bernoilli random variable.\n",
    "Call this distribution $\\operatorname{Bernoulli}(p)$, which then has mean $p$ and variance $p(1-p)$.\n",
    "The value of a Bernoulli random variable is 1 with probability $p$ and 0 with probability $1-p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we talk about the quantity $X_1 + X_2 + \\dots + X_n$ when each $X_i$ is Bernoulli$(p)$ is called the Binomial distribution, denote as $\\operatorname{Binom(n,p)}$.\n",
    "The mean of a Binomial$(n,p)$ is\n",
    "$$ \\mathbb{E}(X_1 + X_2 + \\dots + X_N) = \\mathbb{E}(X_1) + \\dots + \\mathbb{E}(X_N) = np $$\n",
    "The variance is then\n",
    "$$\n",
    "\\operatorname{Var}(X_1 + X_2 + \\dots + X_N) = n \\operatorname{Var}(X_1) = np(1-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the distribution of estimated mean will behave like $\\mathcal{N}(np, np(1-p))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "N = 100\n",
    "bias = 0.5\n",
    "num_trials = 10000\n",
    "\n",
    "def binom_draw(n,p):\n",
    "    \"\"\"Generate one draw from a Binomial(n,p) distribution\"\"\"\n",
    "    return np.sum(np.random.choice([0,1],size=n,p=[1-p,p]))\n",
    "# see next cell for information about the choice function\n",
    "\n",
    "observations = [binom_draw(N,bias) for _ in range(num_trials)]\n",
    "\n",
    "# want to plot the distribution of these observations\n",
    "counts = Counter(observations)\n",
    "print(counts)\n",
    "\n",
    "plt.bar([x for x in counts.keys()], [v / num_trials for v in counts.values()])\n",
    "\n",
    "# now compare with actual normal pdf\n",
    "mu = N*bias\n",
    "sigma = math.sqrt(N*bias*(1-bias))\n",
    "\n",
    "xs = np.arange(30,70,0.1)\n",
    "plt.plot(xs, [normal_pdf(x,mu,sigma) for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.random.choice)\n",
    "np.random.choice([0,1],p=[0.5, 0.5],size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
